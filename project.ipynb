{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a0f03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import wfdb\n",
    "import pandas as pd\n",
    "\n",
    "# Device setup\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50caf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Extract beats from a record\n",
    "# ----------------------------\n",
    "def extract_beats(record_name: str, window: int = 360, db_path: str = \"./data/mitdb/1.0.0/\"):\n",
    "    \"\"\"\n",
    "    Extract beats and annotations from a single MIT-BIH record\n",
    "    \"\"\"\n",
    "    record = wfdb.rdrecord(f\"{db_path}/{record_name}\")\n",
    "    annotation = wfdb.rdann(f\"{db_path}/{record_name}\", 'atr')\n",
    "    signal = record.p_signal[:, 0]  # use lead 0\n",
    "    beats, labels = [], []\n",
    "\n",
    "    for i, ann_sample in enumerate(annotation.sample):\n",
    "        if ann_sample - window < 0 or ann_sample + window >= len(signal):\n",
    "            continue\n",
    "        beat = signal[ann_sample - window: ann_sample + window]\n",
    "        beats.append(beat)\n",
    "        labels.append(annotation.symbol[i])\n",
    "\n",
    "    return np.array(beats), np.array(labels)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Load multiple records\n",
    "# ----------------------------\n",
    "def load_dataset(records, window=360, db_path=\"data/physionet.org/files/mitdb/1.0.0/\"):\n",
    "    \"\"\"\n",
    "    Load multiple records into X (beats) and y (labels)\n",
    "    \"\"\"\n",
    "    all_beats, all_labels = [], []\n",
    "    for rec in records:\n",
    "        print(f\"Extracting {rec}...\")\n",
    "        beats, labels = extract_beats(rec, window, db_path)\n",
    "        all_beats.append(beats)\n",
    "        all_labels.append(labels)\n",
    "    X = np.concatenate(all_beats, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Normalize beats\n",
    "# ----------------------------\n",
    "def normalize(X):\n",
    "    \"\"\"Z-score normalization per beat\"\"\"\n",
    "    return (X - X.mean(axis=1, keepdims=True)) / (X.std(axis=1, keepdims=True) + 1e-6)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Train/Val/Test split\n",
    "# ----------------------------\n",
    "def split_data(X, y, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Stratified split into train/val/test sets\n",
    "    \"\"\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Format for CNN & LSTM\n",
    "# ----------------------------\n",
    "def prepare_for_cnn(X):\n",
    "    \"\"\"(samples, length, channels) -> (samples, 1, length)\"\"\"\n",
    "    return X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "def prepare_for_lstm(X):\n",
    "    \"\"\"(samples, length, channels) -> (samples, length, 1)\"\"\"\n",
    "    return X.reshape((X.shape[0], X.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6952b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Filter & encode labels\n",
    "# ----------------------------\n",
    "def filter_and_encode_labels(y: np.ndarray, min_count: int = 5):\n",
    "    \"\"\"\n",
    "    Filters rare labels and encodes remaining labels to integers\n",
    "    \"\"\"\n",
    "    counter = Counter(y)\n",
    "    keep = {lbl for lbl, c in counter.items() if c >= min_count}\n",
    "    mask = np.array([lbl in keep for lbl in y])\n",
    "    y_filtered = y[mask]\n",
    "    unique = sorted(list({lbl for lbl in y_filtered}))\n",
    "    label_to_int = {lbl: i for i, lbl in enumerate(unique)}\n",
    "    y_encoded = np.array([label_to_int[lbl] for lbl in y_filtered])\n",
    "    return y_encoded, label_to_int, mask\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ECG Dataset\n",
    "# ----------------------------\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare data for CNN\n",
    "# ----------------------------\n",
    "def prepare_for_cnn(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reshape X to (samples, length, 1) for collate_cnn to permute to (batch, 1, length)\n",
    "    \"\"\"\n",
    "    return X.reshape((X.shape[0], X.shape[1], 1))  # (N, L, 1)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Collate function\n",
    "# ----------------------------\n",
    "def collate_cnn(batch):\n",
    "    \"\"\"\n",
    "    Convert list of (x, y) to (batch, 1, L) for Conv1d\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    xs = torch.stack(xs)          # (batch, L, 1)\n",
    "    xs = xs.permute(0, 2, 1)      # (batch, 1, L) âœ… channels first\n",
    "    ys = torch.stack([y for y in ys])\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def collate_lstm(batch):\n",
    "    xs, ys = zip(*batch)\n",
    "    xs = torch.stack(xs)\n",
    "    ys = torch.stack(ys)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Checkpoint functions\n",
    "# ----------------------------\n",
    "def save_checkpoint(state, path):\n",
    "    torch.save(state, path)\n",
    "\n",
    "def load_checkpoint(path: str, device: str = \"cpu\"):\n",
    "    import torch\n",
    "    # safe load with full checkpoint (not weights-only)\n",
    "    return torch.load(path, map_location=device, weights_only=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df904c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN\n",
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self, seq_len, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Simple LSTM\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, seq_len, n_classes, hidden_size=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last timestep\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62dbed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 100...\n",
      "Extracting 101...\n",
      "Extracting 102...\n",
      "Extracting 103...\n",
      "Extracting 104...\n",
      "Extracting 105...\n",
      "Extracting 106...\n",
      "Extracting 107...\n",
      "Extracting 108...\n",
      "Extracting 109...\n",
      "Extracting 111...\n",
      "Extracting 112...\n",
      "Extracting 113...\n",
      "Extracting 114...\n",
      "Extracting 115...\n",
      "Extracting 116...\n",
      "Extracting 117...\n",
      "Extracting 118...\n",
      "Extracting 119...\n",
      "Extracting 121...\n",
      "Extracting 122...\n",
      "Extracting 123...\n",
      "Extracting 124...\n",
      "Extracting 200...\n",
      "Extracting 201...\n",
      "Extracting 202...\n",
      "Extracting 203...\n",
      "Extracting 205...\n",
      "Extracting 207...\n",
      "Extracting 208...\n",
      "Extracting 209...\n",
      "Extracting 210...\n",
      "Extracting 212...\n",
      "Extracting 213...\n",
      "Extracting 214...\n",
      "Extracting 215...\n",
      "Extracting 219...\n",
      "Extracting 220...\n",
      "Extracting 221...\n",
      "Extracting 222...\n",
      "Extracting 223...\n",
      "Extracting 228...\n",
      "Extracting 230...\n",
      "Extracting 231...\n",
      "Extracting 232...\n",
      "Extracting 233...\n",
      "Extracting 234...\n",
      "Shapes:\n",
      "CNN Train: (88136, 720, 1) LSTM Train: (88136, 720, 1)\n",
      "Num classes: 19\n"
     ]
    }
   ],
   "source": [
    "# MIT-BIH record splits\n",
    "train_records = [\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\n",
    "                 \"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\"117\",\"118\",\"119\",\"121\",\"122\",\"123\",\"124\"]\n",
    "test_records = [\"200\",\"201\",\"202\",\"203\",\"205\",\"207\",\"208\",\"209\",\"210\",\"212\",\n",
    "                \"213\",\"214\",\"215\",\"219\",\"220\",\"221\",\"222\",\"223\",\"228\",\"230\",\"231\",\"232\",\"233\",\"234\"]\n",
    "\n",
    "# Load and normalize\n",
    "X_train_raw, y_train_raw = load_dataset(train_records)\n",
    "X_test_raw, y_test_raw = load_dataset(test_records)\n",
    "X = np.concatenate([X_train_raw, X_test_raw])\n",
    "y = np.concatenate([y_train_raw, y_test_raw])\n",
    "X = normalize(X)\n",
    "\n",
    "# Filter & encode\n",
    "y, label_map, mask = filter_and_encode_labels(y, min_count=20)\n",
    "X = X[mask]\n",
    "\n",
    "# Split\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "\n",
    "# Format for models\n",
    "X_train_cnn = prepare_for_cnn(X_train)\n",
    "X_val_cnn = prepare_for_cnn(X_val)\n",
    "X_test_cnn = prepare_for_cnn(X_test)\n",
    "\n",
    "X_train_lstm = prepare_for_lstm(X_train)\n",
    "X_val_lstm = prepare_for_lstm(X_val)\n",
    "X_test_lstm = prepare_for_lstm(X_test)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"CNN Train:\", X_train_cnn.shape, \"LSTM Train:\", X_train_lstm.shape)\n",
    "print(\"Num classes:\", len(label_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name=\"cnn\", epochs=10, batch_size=128, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model (CNN or LSTM)\n",
    "    Returns best checkpoint path and metrics\n",
    "    \"\"\"\n",
    "    if model_name==\"cnn\":\n",
    "        collate_fn = collate_cnn\n",
    "        X_tr, X_val_use = X_train_cnn, X_val_cnn\n",
    "    else:\n",
    "        collate_fn = collate_lstm\n",
    "        X_tr, X_val_use = X_train_lstm, X_val_lstm\n",
    "\n",
    "    train_ds = ECGDataset(X_tr, y_train)\n",
    "    val_ds = ECGDataset(X_val_use, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    n_classes = len(label_map)\n",
    "    seq_len = X.shape[1]\n",
    "    model = Simple1DCNN(seq_len, n_classes).to(DEVICE) if model_name==\"cnn\" else SimpleLSTM(seq_len, n_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    best_path = f\"best_{model_name}.pth\"\n",
    "    metrics_history = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        t0 = time.time()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                logits = model(xb)\n",
    "                preds.append(torch.argmax(logits, 1).cpu().numpy())\n",
    "                trues.append(yb.cpu().numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        trues = np.concatenate(trues)\n",
    "        val_acc = accuracy_score(trues, preds)\n",
    "        metrics_history.append((epoch, np.mean(losses), val_acc))\n",
    "\n",
    "        print(f\"{model_name.upper()} Epoch {epoch}: Loss={np.mean(losses):.4f} ValAcc={val_acc:.4f} Time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_checkpoint({\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"label_map\": label_map\n",
    "            }, best_path)\n",
    "\n",
    "    return best_path, metrics_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 1: Loss=1.2852 ValAcc=0.6911 Time=197.3s\n",
      "LSTM Test Accuracy: 0.6873865698729582\n"
     ]
    }
   ],
   "source": [
    "best_lstm_path, lstm_metrics = train_model(\"lstm\", epochs=5)\n",
    "\n",
    "# Evaluate\n",
    "val_ds = ECGDataset(X_test_lstm, y_test)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate_lstm)\n",
    "\n",
    "model = SimpleLSTM(X.shape[1], len(label_map)).to(DEVICE)\n",
    "checkpoint = load_checkpoint(best_lstm_path, DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds.append(torch.argmax(logits, 1).cpu().numpy())\n",
    "        trues.append(yb.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds)\n",
    "trues = np.concatenate(trues)\n",
    "lstm_acc = accuracy_score(trues, preds)\n",
    "print(\"LSTM Test Accuracy:\", lstm_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e1119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Epoch 1: Loss=0.8500 ValAcc=0.8377 Time=155.5s\n",
      "CNN Test Accuracy: 0.838929219600726\n"
     ]
    }
   ],
   "source": [
    "best_cnn_path, cnn_metrics = train_model(\"cnn\", epochs=5)\n",
    "\n",
    "# Evaluate\n",
    "val_ds = ECGDataset(X_test_cnn, y_test)\n",
    "val_loader = DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate_cnn)\n",
    "\n",
    "model = Simple1DCNN(X.shape[1], len(label_map)).to(DEVICE)\n",
    "checkpoint = load_checkpoint(best_cnn_path, DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds.append(torch.argmax(logits, 1).cpu().numpy())\n",
    "        trues.append(yb.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(preds)\n",
    "trues = np.concatenate(trues)\n",
    "cnn_acc = accuracy_score(trues, preds)\n",
    "print(\"CNN Test Accuracy:\", cnn_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a522679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Best Epoch Loss</th>\n",
       "      <th>Best Epoch Val Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.687387</td>\n",
       "      <td>1.285218</td>\n",
       "      <td>0.691125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.838929</td>\n",
       "      <td>0.849978</td>\n",
       "      <td>0.837721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Test Accuracy  Best Epoch Loss  Best Epoch Val Acc\n",
       "0  LSTM       0.687387         1.285218            0.691125\n",
       "1   CNN       0.838929         0.849978            0.837721"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\": [\"LSTM\", \"CNN\"],\n",
    "    \"Test Accuracy\": [lstm_acc, cnn_acc],\n",
    "    \"Best Epoch Loss\": [min([m[1] for m in lstm_metrics]), min([m[1] for m in cnn_metrics])],\n",
    "    \"Best Epoch Val Acc\": [max([m[2] for m in lstm_metrics]), max([m[2] for m in cnn_metrics])]\n",
    "})\n",
    "\n",
    "metrics_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
